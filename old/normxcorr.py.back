"""

class NormXCorr(nn.Module):

    def __init__(self):

        super(NormXCorr, self).__init__()

    def forward(self, X_, Y_):

        return normxcorr.apply(X_, Y_)
"""

"""
#Basically same as above, but defined as Custom Pytorch Module
#Not computationally efficient in this case

class NormXCorr(nn.Module):

    def __init__(self,  ):

        super(NormXCorr, self).__init__()

    def forward(self, X_, Y_, patch_size = 5, stride =1, epsilon = 0.01):

        \"""
        - data[1] output of one pipeline
        - data[0] output of the other

        Following the paper terminology:
        data[1] = feature map X
        data[0] = feature map Y

        Except here grouped in batches
        \"""

        #X_ = torch.tensor(X)
        #Y_ = torch.tensor(Y)

        sample_size = X_.shape[0]
        in_depth = X_.shape[1]
        in_height= X_.shape[2]
        in_width= X_.shape[3]

        out_depth= patch_size*in_width*in_depth

        d = int(patch_size/2)


        \"""
            for each depth i in range(25):

                1. take the ith 37x12 feature map from X
                    1.a create copy of X with padding of two at each margin -> 41x16
                2. take the ith 37x12 feature map from Y
                    2.a create copy of Y with same size of 1.a, but extra vertical padding of two each -> 45x16
        \"""

        #output = []

        X_pad = F.pad(X_, (d, d, d, d))


        Y_pad = F.pad(Y_, (d, d, 2 * d, 2 * d))

        # Empty matrix to preserve original positions

        M_out = Variable(X_.new_zeros([sample_size, in_depth, in_width * patch_size, in_height, in_width],
                                   dtype=torch.float32)).to(device, non_blocking=True)  # batch_size x 60 x 37 x 12

        for i in range(in_depth):

            X_i = X_pad[:, i, :]
            Y_i = Y_pad[:, i, :]

            #3.  For each jth pixel in X:
            for y,x in itertools.product(range(d, in_height+d), range(d,in_width+d)):

                y_orig = y-d
                x_orig = x-d

                #3.a take related 5x5 patch in 1.a (E)

                E_ = X_i[:, y-d:y+d+1, x-d:x+d+1]


                E_ = E_.reshape((sample_size, E_.shape[1] * E_.shape[2])) # batch_size x 25

                E_mean = E_.mean(1, keepdim=True)
                E_std = E_.std(1, keepdim=True)

                E_norm = (E_ - E_mean) / ((E_std + epsilon) * (E_.shape[1]-1))

                #print(E_.shape)
                #print(E_mean.shape)

                # Wider rectangular (inexact) search area, width is fixed at 12

                #Extra padding of 2 to extract 5x5 patches
                padded_width = Y_i.shape[1]

                rect_area = Y_i[:, y-d:y + 3*d + 1, d-2:padded_width+1]
                #print(rect_area.shape)

                # 3.b take all possible 5x5 patches in 2.a. (all possible Fs)

                F_values = rect_area.unfold(1, patch_size, stride).unfold(2, patch_size, stride)
                # print(F_values.shape) # batch_size x 5 x 12 x 5 x 5

                #3.c Init empty output vec of (area,1), where area = rect_area(3.b) -> (60,1)

                normxcorrs = self.compute_normxcorr(E_norm, F_values, patch_size, in_width, epsilon)

                # print(normxcorr_values.shape) #batch_sizex1 , i.e., each similarity value is a scalar
                # stack normxcorr_values, there will be 60 in the end

                normxcorrs = torch.stack(normxcorrs, 1)

                #print(normxcorrs.shape) # shape batch_sizex 60 x 1

                M_out[:,i, :, y_orig, x_orig] = normxcorrs  # Assign to original position in feature map

            #for all y and x
            # This for ends with a 37x12x60 matrix

        #print(len(output))  #25
        #for all depths, yielding output -> 37x12x60*25 = 37x12x1500
        #print(M_out.shape)

        return M_out.reshape((sample_size, out_depth, in_height, in_width))


    def compute_normxcorr(self, E_norm, F_values, patch_size, in_width, epsilon):

        \"""
            3.d for each E,F pair:
                - reshape both to be (25,) --> E',F'
                - compute mean of E' and F'
                - compute std devs of E' and F'
                - add 0.01 to each std dev to avoid division by 0
                - compute normcrosscorr between E'F'
        \"""
        normxcorrs = [] #Variable(X.new_zeros([batch_size, in_depth, in_width * patch_size, in_height, in_width],
                                  # dtype=torch.float32)).to(device)       ###    #[]


        # For each patch in rectangular search window: 5x12 patches of size 5x5 each

        for y__, x__ in itertools.product(range(patch_size), range(in_width)):

            F_ = F_values[:, y__, x__, : ]

            #print(F_.shape)  # batch_sizex5x5
            #print(E_.shape)

            F_ = F_.reshape((F_.shape[0], F_.shape[1] * F_.shape[2]))

            #print(F_.shape)  # batch_sizex25, i.e., N-dimensional vector

            # Batch-wise computations

            \"""
            F_norm = E_norm.clone().detach()
            F.normalize(F_, p=2, dim=-1, eps=epsilon, out=F_norm)

            \"""

            F_mean = F_.mean(1, keepdim=True)
            #print(len(F_mean.tolist()) )#batch_sizex1

            F_std = F_.std(1, keepdim=True) + epsilon

            #F_norm = transforms.ToTensor(inv_normalize)

            #print(F_norm.shape)
            #print(F_norm[0])

            F_norm = (F_ - F_mean) / (F_std)

            #print(F_norm2.shape)
            #print(F_norm2[0])
            #print((E_norm * F_norm).shape)

            normxcorrs.append(torch.sum(E_norm * F_norm, 1))

        return normxcorrs

"""